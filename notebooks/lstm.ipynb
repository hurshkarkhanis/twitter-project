{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(filename):\n",
    "    out = pd.read_csv(filename, encoding=\"ISO-8859-1\", names=['polarity', 'id', 'date', 'query', 'user', 'text'])\n",
    "    out = out.drop(labels=['id', 'date', 'query', 'user'], axis = 1)\n",
    "    out = out.loc[out['polarity'] != 2]\n",
    "    out['polarity'] = out['polarity'].replace(4, 1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_df('../trainingandtestdata/training.1600000.processed.noemoticon.csv')\n",
    "test = clean_df('../trainingandtestdata/testdata.manual.2009.06.14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])\n",
    "data = data.sample(n=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y = data['polarity']\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "127/127 [==============================] - 170s 1s/step - loss: -14.9907 - accuracy: 0.0021 - val_loss: -24.7740 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 134s 1s/step - loss: -30.7923 - accuracy: 0.0000e+00 - val_loss: -38.4910 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 96s 759ms/step - loss: -43.8146 - accuracy: 0.0000e+00 - val_loss: -51.6008 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 70s 554ms/step - loss: -56.4769 - accuracy: 0.0000e+00 - val_loss: -64.5893 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 69s 543ms/step - loss: -68.9935 - accuracy: 0.0000e+00 - val_loss: -77.3647 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 69s 547ms/step - loss: -81.4927 - accuracy: 0.0000e+00 - val_loss: -90.3378 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 88s 693ms/step - loss: -94.2102 - accuracy: 0.0000e+00 - val_loss: -103.2043 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 86s 678ms/step - loss: -106.7552 - accuracy: 0.0000e+00 - val_loss: -116.1094 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 74s 585ms/step - loss: -119.9364 - accuracy: 0.0000e+00 - val_loss: -127.6877 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 70s 551ms/step - loss: -145.9198 - accuracy: 0.0000e+00 - val_loss: -132.2834 - val_accuracy: 0.0022\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------BUILDING MODEL\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.00001)])\n",
    "\n",
    "# ---------------------------PLOTTING RESULTS\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].plot(history.history['loss'], label='train', color ='#E43C40')\n",
    "ax[0].plot(history.history['val_loss'], label='test', color ='#00B2A9')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].plot(history.history['accuracy'], label='train', color ='#E43C40')\n",
    "ax[1].plot(history.history['val_accuracy'], label='test', color ='#00B2A9')\n",
    "ax[1].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
